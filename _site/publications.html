<!DOCTYPE html>
<html lang="en">
<!-- Template: https://github.com/luost26/academic-homepage -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Publications - </title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"> -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Raleway:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/global.css">
</head>
<body class="bg-light" data-spy="scroll" data-target="#navbar-year" data-offset="100">
    <nav class="navbar navbar-expand-sm navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container">
        <a class="navbar-brand"><strong></strong></a>
        <button class="navbar-toggler" style="font-size: 1em; padding: 0.5em;" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fas fa-map"></i> Menu
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item ">
                    <a class="nav-link" href="/">Home</a>
                </li>
                
                <li class="nav-item active">
                    <a class="nav-link" href="/publications">Publications</a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

    <div class="container">
        

<div class="row">
    <div class="col-12 col-lg-10">
        
        
        <h2 class="pt-4" id="year-2025">2025</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-3">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/dufal.png" alt="DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">TMLR</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Cuong Tran Van, </span><span class="text-body">
            Trong-Thang Pham, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Duy Minh Ho Nguyen, </span><span class="text-body">
            Ngan Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Transactions on Machine Learning Research (TMLR)</i> 2025  <span class="badge badge-pill badge-publication badge-success">J2C Certification</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://openreview.net/pdf?id=2wAZjAtK16">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/UARK-AICV/DuFal">[Code]</a>
                
                
                
                <a target="_blank" href="https://drive.google.com/file/d/1v5V8T7o3bslzauiQJsGVoeG_MNlaSq4u/view">[Video]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-3"><img data-src="/assets/images/covers/dufal.png" alt="DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">TMLR</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Cuong Tran Van, </span><span class="text-body">
            Trong-Thang Pham, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Duy Minh Ho Nguyen, </span><span class="text-body">
            Ngan Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Transactions on Machine Learning Research (TMLR)</i> 2025  <span class="badge badge-pill badge-publication badge-success">J2C Certification</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://openreview.net/pdf?id=2wAZjAtK16">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/UARK-AICV/DuFal">[Code]</a>
                
                
                
                <a target="_blank" href="https://drive.google.com/file/d/1v5V8T7o3bslzauiQJsGVoeG_MNlaSq4u/view">[Video]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-7">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/diflowdubber.png" alt="DiFlowDubber: Discrete Flow Matching for Automated Video Dubbing via Cross-Modal Alignment and Synchronization" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">DiFlowDubber: Discrete Flow Matching for Automated Video Dubbing via Cross-Modal Alignment and Synchronization</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            Jeongsoo Choi, </span><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-7"><img data-src="/assets/images/covers/diflowdubber.png" alt="DiFlowDubber: Discrete Flow Matching for Automated Video Dubbing via Cross-Modal Alignment and Synchronization" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">DiFlowDubber: Discrete Flow Matching for Automated Video Dubbing via Cross-Modal Alignment and Synchronization</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            Jeongsoo Choi, </span><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-4">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/flamed-tts.png" alt="Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Huynh Nguyen Dang, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2510.02848">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/flamed-tts/Flamed-TTS">[Code]</a>
                
                
                
                <a target="_blank" href="https://flamed-tts.github.io/">[Project]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-4"><img data-src="/assets/images/covers/flamed-tts.png" alt="Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Huynh Nguyen Dang, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2510.02848">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/flamed-tts/Flamed-TTS">[Code]</a>
                
                
                
                <a target="_blank" href="https://flamed-tts.github.io/">[Project]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-6">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/diflow-tts.png" alt="DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2509.09631">[Paper]</a>
                
                
                
                <a target="_blank" href="https://diflow-tts.github.io/">[Project]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-6"><img data-src="/assets/images/covers/diflow-tts.png" alt="DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            Hieu-Nghia Huynh-Nguyen, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2509.09631">[Paper]</a>
                
                
                
                <a target="_blank" href="https://diflow-tts.github.io/">[Project]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-2">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/ct-scangaze.png" alt="CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">ICCV</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Trong Thang Pham, </span><span class="text-body">
            Akash Awasthi, </span><span class="text-body">
            Saba Khan, </span><span class="text-body">
            Esteban Duran Marti, </span><span class="text-body">
            Tien-Phat Nguyen, </span><span class="text-body">
            Khoa Vo, </span><span class="text-body">
            Minh Tran, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Cuong Tran, </span><span class="text-body">
            Yuki Ikebe, </span><span class="text-body">
            Anh Totti Nguyen, </span><span class="text-body">
            Anh Nguyen, </span><span class="text-body">
            Zhigang Deng, </span><span class="text-body">
            Carol C Wu, </span><span class="text-body">
            Hien Nguyen, </span><span class="text-body">
            Ngan Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</i> 2025  <span class="badge badge-pill badge-publication badge-success">Highlight</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/UARK-AICV/CTScanGaze">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-2"><img data-src="/assets/images/covers/ct-scangaze.png" alt="CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">ICCV</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Trong Thang Pham, </span><span class="text-body">
            Akash Awasthi, </span><span class="text-body">
            Saba Khan, </span><span class="text-body">
            Esteban Duran Marti, </span><span class="text-body">
            Tien-Phat Nguyen, </span><span class="text-body">
            Khoa Vo, </span><span class="text-body">
            Minh Tran, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Cuong Tran, </span><span class="text-body">
            Yuki Ikebe, </span><span class="text-body">
            Anh Totti Nguyen, </span><span class="text-body">
            Anh Nguyen, </span><span class="text-body">
            Zhigang Deng, </span><span class="text-body">
            Carol C Wu, </span><span class="text-body">
            Hien Nguyen, </span><span class="text-body">
            Ngan Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</i> 2025  <span class="badge badge-pill badge-publication badge-success">Highlight</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Pham_CT-ScanGaze_A_Dataset_and_Baselines_for_3D_Volumetric_Scanpath_Modeling_ICCV_2025_paper.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/UARK-AICV/CTScanGaze">[Code]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-5">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/a2v.jpg" alt="Precise Video-to-Audio Generation with Cross-Modal Alignment in Latent Space" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">Precise Video-to-Audio Generation with Cross-Modal Alignment in Latent Space</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Luong Tran, </span><span class="text-body">
            Long-Khanh Pham, </span><span class="text-body">
            Paarth Neekhara, </span><span class="text-body">
            Shehzeen Samarah Hussain, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-5"><img data-src="/assets/images/covers/a2v.jpg" alt="Precise Video-to-Audio Generation with Cross-Modal Alignment in Latent Space" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">Under Review</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">Precise Video-to-Audio Generation with Cross-Modal Alignment in Latent Space</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Thanh V. T. Tran, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Luong Tran, </span><span class="text-body">
            Long-Khanh Pham, </span><span class="text-body">
            Paarth Neekhara, </span><span class="text-body">
            Shehzeen Samarah Hussain, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Under Review</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2025-pub-1">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/ozspeech.png" alt="OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">ACL</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Nghia Huynh Nguyen Hieu, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Huynh Nguyen Dang, </span><span class="text-body">
            Thieu Vo, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.acl-long.1043.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/ozspeech/OZSpeech">[Code]</a>
                
                
                
                <a target="_blank" href="https://ozspeech.github.io/OZSpeech_Web/">[Project]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2025-pub-1"><img data-src="/assets/images/covers/ozspeech.png" alt="OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">ACL</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Nghia Huynh Nguyen Hieu, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Huynh Nguyen Dang, </span><span class="text-body">
            Thieu Vo, </span><span class="text-body">
            Truong-Son Hy, </span><span class="text-body">
            Van Nguyen</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)</i> 2025 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://aclanthology.org/2025.acl-long.1043.pdf">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/ozspeech/OZSpeech">[Code]</a>
                
                
                
                <a target="_blank" href="https://ozspeech.github.io/OZSpeech_Web/">[Project]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
        </div>
        
        
        <h2 class="pt-4" id="year-2024">2024</h2>
        <div class="my-0 p-0 bg-white shadow-sm rounded-sm">
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2024-pub-2">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/lightgpt.png" alt="LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">arXiv</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Khai Le-Duc*, </span><span class="text-body">
            Ryan Zhang*, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen*</strong>, </span><span class="text-body">
            Tan-Hanh Pham, </span><span class="text-body">
            Anh Dao, </span><span class="text-body">
            Ba Hung Ngo, </span><span class="text-body">
            Anh Totti Nguyen, </span><span class="text-body">
            Truong-Son Hy</span>
<mark>(* <i> equal contribution</i>)</mark></p>
            
            <p class="mt-0 mb-0 small">
                <i>Preprint</i> 2024 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2407.12064">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/nngocson2002/LVLM-Med">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2024-pub-2"><img data-src="/assets/images/covers/lightgpt.png" alt="LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">arXiv</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            Khai Le-Duc*, </span><span class="text-body">
            Ryan Zhang*, </span><span class="text-body">
            <strong>Ngoc-Son Nguyen*</strong>, </span><span class="text-body">
            Tan-Hanh Pham, </span><span class="text-body">
            Anh Dao, </span><span class="text-body">
            Ba Hung Ngo, </span><span class="text-body">
            Anh Totti Nguyen, </span><span class="text-body">
            Truong-Son Hy</span>
<mark>(* <i> equal contribution</i>)</mark></p>
            
            <p class="mt-0 mb-0 small">
                <i>Preprint</i> 2024 
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2407.12064">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/nngocson2002/LVLM-Med">[Code]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
                


<!-- Desktop版本 -->
<div class="d-none d-md-block" data-paper-id="2024-pub-1">
    <div class="row no-gutters border-bottom border-gray"><div class="col-md-3 col-xl-2 mb-md-0 p-md-3" style="position: relative;">
            <img data-src="/assets/images/covers/avivqa.jpg" alt="Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="lazy w-100 rounded-sm" src="/assets/images/empty_300x200.png"><span class="pub-badge">Elsevier Journal</span></div>
        <div class="col-md-9 col-xl-10 p-3 pl-md-0"><h5 class="mt-0 mb-1 font-weight-normal">Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Van Son Nguyen, </span><span class="text-body">
            Tung Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Journal Computers and Electrical Engineering</i> 2024  <span class="badge badge-pill badge-publication badge-success">Q1, IF = 4.9</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2407.21229">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/nngocson2002/ViVQA">[Code]</a>
                
                
            </p>

        </div>
    </div>
</div>

<!-- Mobile版本 -->
<div class="card w-100 d-md-none border-top-0 border-right-0 border-left-0 rounded-0" style="overflow-y: scroll; position: relative;" data-paper-id="2024-pub-1"><img data-src="/assets/images/covers/avivqa.jpg" alt="Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="lazy card-img w-100" src="/assets/images/empty_300x200.png"><span class="pub-badge">Elsevier Journal</span><div class="card-img-overlay d-flex align-items-start flex-column" style="background-color: rgba(255, 255, 255, 0.85);">
        <div class="mb-auto"></div>
        <div>
            <h5 class="mt-0 mb-1 font-weight-normal">Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration</h5>
            <p class="mt-0 mb-0 small"><span class="text-body">
            <strong>Ngoc-Son Nguyen</strong>, </span><span class="text-body">
            Van Son Nguyen, </span><span class="text-body">
            Tung Le</span></p>
            
            <p class="mt-0 mb-0 small">
                <i>Journal Computers and Electrical Engineering</i> 2024  <span class="badge badge-pill badge-publication badge-success">Q1, IF = 4.9</span>
                <span class="citation-badge"></span>
            </p>
            
            <p class="mt-0 mb-0 small text-muted d-none d-sm-block"></p>
            
            <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
                
                
                <a target="_blank" href="https://arxiv.org/pdf/2407.21229">[Paper]</a>
                
                
                
                <a target="_blank" href="https://github.com/nngocson2002/ViVQA">[Code]</a>
                
                
            </p>
        </div>
    </div>
</div>
            
        </div>
        
    </div>

    <div class="col-2 d-none d-lg-block">
        <div id="navbar-year" class="nav nav-pills flex-column sticky-top" style="top: 80px">
            
            <a class="nav-link d-block" href="#year-2025">2025</a>
            
            <a class="nav-link d-block" href="#year-2024">2024</a>
            
        </div>
    </div>

</div>

    </div>
    <footer class="footer">
    <div class="container">
        <div class="row my-3">
            <div class="col-6">
                <div class="text-muted">
                    <i>Last updated: Jan 2026</i>
                </div>
            </div>
            <div class="col-6">
                <div class="text-right text-muted">
                    <a href="https://github.com/luost26/academic-homepage" target="_blank"><i class="fas fa-pencil-ruler"></i> academic-homepage</a>
                </div>
            </div>
        </div>
    </div>
</footer>


    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
    <script src="/assets/js/common.js"></script>
    <style>
.badge {
    display: inline-block;
    padding: 0.25em 0.4em;
    font-size: 0.75rem;
    font-weight: 700;
    line-height: 1;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
    border-radius: 0.25rem;
}

.pub-badge {
    position: absolute;
    top: 8px;
    left: 8px;
    background-color: #0E2C6E;
    color: white;
    padding: 2px 8px;
    border-radius: 3px;
    font-size: 0.7em;
    font-weight: bold;
    z-index: 10;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
}

.badge-publication {
    font-size: 0.75rem;
    border-radius: 10rem;
    margin-left: 0.25rem;
    margin-right: 0.25rem;
}

.badge-pill {
    padding-right: 0.6em;
    padding-left: 0.6em;
    border-radius: 10rem;
}

.badge-info {
    color: #fff;
    background-color: #17a2b8;
}

.citation-badge {
    display: inline-block;
    margin-left: 0.25rem;
}

.citation-badge a {
    text-decoration: none;
    transition: opacity 0.3s ease;
}

.citation-badge a:hover {
    opacity: 0.85;
}
</style>

<script>
(function() {
    if (window._citationsLoaded) return;
    window._citationsLoaded = true;
    
    async function loadCitations() {
        try {
            const response = await fetch('/assets/data/publications.json');
            if (!response.ok) return;
            const data = await response.json();
            
            document.querySelectorAll('[data-paper-id]').forEach(el => {
                const id = el.getAttribute('data-paper-id');
                const badge = el.querySelector('.citation-badge');
                if (!badge) return;
                
                const paper = data.papers.find(p => p.id === id);
                if (!paper || paper.citations == null) {
                    badge.style.display = 'none';
                    return;
                }
                
                const s2Id = paper.semantic_scholar?.paper_id;
                const count = paper.citations;
                const label = count === 1 ? 'citation' : 'citations';
                
                if (s2Id) {
                    badge.innerHTML = `<a href="https://www.semanticscholar.org/paper/${s2Id}" target="_blank" class="badge badge-pill badge-info" style="color:#fff;text-decoration:none;font-size:0.75rem;"><i class="fas fa-quote-right"></i> ${count} ${label}</a>`;
                } else {
                    badge.innerHTML = `<span class="badge badge-pill badge-info" style="font-size:0.75rem;"><i class="fas fa-quote-right"></i> ${count} ${label}</span>`;
                }
            });
        } catch (e) {
            console.error('Citation load error:', e);
        }
    }
    
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', loadCitations);
    } else {
        loadCitations();
    }
})();
</script>

</body>
</html>
